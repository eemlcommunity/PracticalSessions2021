{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ViT Tutorial (EEML21) - Practical Part",
      "provenance": [
        {
          "file_id": "1dd_mAAVFqKIhxcB7GhOn_X2l1HQ63QI1",
          "timestamp": 1625668556805
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhZm0kpPpH6"
      },
      "source": [
        "##### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KfmzfvFxPuk7"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVCm4CnP1Do"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2021/blob/main/vision/vision_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOn90tXAeh9J"
      },
      "source": [
        "# ViT Tutorial\n",
        "\n",
        "This the Colab for the \"ViT Tutorial\" at [EEML 2021](https://virtual.eeml.eu/).\n",
        "\n",
        "See also slides at [ViT Tutorial (EEML21)](https://github.com/eemlcommunity/PracticalSessions2021/blob/main/vision/vision_transformers.pdf).\n",
        "\n",
        "Note: This Colab assumes that you already walked through the EEML introduction Colab at\n",
        "https://colab.sandbox.google.com/github/eemlcommunity/PracticalSessions2021/blob/main/intro/intro_tutorial.ipynb\n",
        "\n",
        "**Exercises**:\n",
        "Whenever you see a `# YOUR ACTION REQUIRED` please try a couple of minutes to\n",
        "solve the task at hand. You get most out of this tutorial, if you play around\n",
        "with the provided code and see how your changes modify the behavior (i.e.\n",
        "additional to the suggested `# YOUR ACTION REQUIRED` tasks).\n",
        "\n",
        "**Solutions**:\n",
        "This Colab also contains solutions. By default the solutions are\n",
        "hidden and you only see the text \"solution\". In order to see the code for the\n",
        "solutions, you need to double click on those cells. At any point during the\n",
        "tutorial, you can also ask questions (and answer them!) in the EEML Slack\n",
        "channel `#vit`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et9cbiyNba6P"
      },
      "source": [
        "Before you start, you probably want to **make a copy** of this Colab so your\n",
        "changes are not lost:\n",
        "\n",
        "![save a copy](https://i.imgur.com/Ws5KfqV.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1MdLhcPgyW-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoi2gdJZA4b6"
      },
      "source": [
        "# Make sure to use a runtime with GPU!\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq0CqO_bgze0"
      },
      "source": [
        "!pip install -q flax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX-BgTEl0Hn-"
      },
      "source": [
        "from typing import Optional\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import pandas as pd\n",
        "from tqdm import notebook as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trck3mckVuTR"
      },
      "source": [
        "### 0 - Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zeivSGyw4Kq"
      },
      "source": [
        "# Let's say we want to predict garment type from the Fashion-MNIST dataset.\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "ds, info = tfds.load('fashion_mnist', with_info=True)\n",
        "\n",
        "# Show some images, for exploring the dataset check out KYD:\n",
        "# https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=fashion_mnist\n",
        "# (Not too interesting on this toy dataset, but really useful for larger image\n",
        "# datasets...)\n",
        "tfds.show_examples(ds['train'], info, rows=1, cols=5)\n",
        "\n",
        "# Note we have same splits & num_classes as in classical MNIST:\n",
        "info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0fn_0kK2phf"
      },
      "source": [
        "# For convenience.\n",
        "num_classes = info.features['label'].num_classes\n",
        "image_shape = info.features['image'].shape\n",
        "num_test = info.splits['test'].num_examples\n",
        "num_train = info.splits['train'].num_examples\n",
        "\n",
        "def pp(iter):\n",
        "  \"\"\"Preprocesses images/labels for use with JAX.\"\"\"\n",
        "  for batch in iter:\n",
        "    yield (\n",
        "        jnp.array(batch['image']) / 255.,\n",
        "        jax.nn.one_hot(batch['label'], num_classes),\n",
        "    )\n",
        "\n",
        "# Loading entire dataset makes demonstration code simpler, but only works with\n",
        "# small models/datasets...\n",
        "train_images, train_labels = next(pp(iter(ds['train'].batch(num_train))))\n",
        "test_images, test_labels = next(pp(iter(ds['test'].batch(num_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfm6ROfqg1MP"
      },
      "source": [
        "### 1 - Flax Models\n",
        "\n",
        "This sections's content is partly new, partly a repetition of what was\n",
        "already covered in\n",
        "[intro-tutorial: Flax -- alternative library on top of JAX](https://colab.sandbox.google.com/github/eemlcommunity/PracticalSessions2021/blob/main/intro/intro_tutorial.ipynb#scrollTo=ifFR1Iq9YChf)\n",
        "\n",
        "Since you're expected to be new to JAX/Flax, this section explains a couple of\n",
        "core concepts. Of course this is only very shallow introduction! If you're\n",
        "interested in better understanding what's going on please check out the\n",
        "[JAX 101 Colabs](https://jax.readthedocs.io/en/latest/jax-101/)\n",
        "and the\n",
        "[Flax docs](https://flax.readthedocs.io/),\n",
        "maybe starting with the\n",
        "[Annotated MNIST example](https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html)\n",
        "-- after the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20eQx3G-xfvv"
      },
      "source": [
        "#### 1.1 - Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIPNlWKig0lF"
      },
      "source": [
        "# Defining a model is quite simple:\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "  def setup(self):\n",
        "    self.dense = nn.Dense(features=10)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    # Flatten every image, only keeping batch dimension.\n",
        "    x = x.reshape([batch_size, -1])\n",
        "    x = self.dense(x)\n",
        "    # We return normalized logits (\"log_probs\").\n",
        "    return nn.log_softmax(x)\n",
        "\n",
        "# Note how similar the model definition looks to what you would do in PyTorch.\n",
        "# But be aware that the models behave actually quite differently (more on that\n",
        "# below)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-U1mVNygSR"
      },
      "source": [
        "# 1. Initialize the model - this only sets immutable parameters (none in our\n",
        "# simple example).\n",
        "# Note in particular that the `model` instance does NOT include any weights.\n",
        "model = Model()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ALRBGMzqlv"
      },
      "source": [
        "# 2. Initialize model weights - we need fake input for shape inference and\n",
        "# PRNGKey for initializing the weights in a deterministic way.\n",
        "rng = jax.random.PRNGKey(0)\n",
        "variables = model.init(rng, train_images[:1])\n",
        "\n",
        "# Show shape of every parameter.\n",
        "# Note that `nn.Dense()` parameters are stored in the parameter collection\n",
        "# \"dense\" because the field is called `self.dense`\n",
        "jax.tree_map(jnp.shape, variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYO112nbS5rS"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Use `jax.tree_flatten()` to count the total number of parameters of the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "rV4RyfEIS-k4"
      },
      "source": [
        "#@markdown solution - double click to expand\n",
        "sum(jax.tree_flatten(\n",
        "    # Every leaf is a `jnp.ndarray` - first compute then number of parameters in\n",
        "    # every ndarray, then sum them up.\n",
        "    jax.tree_map(lambda p: np.prod(p.shape), variables)\n",
        "# jax.tree_flatten returns both the values and the \"treedef\" that can be used to\n",
        "# reconstruct the tree - we only need the values here.\n",
        ")[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cqNTo0i4XjP"
      },
      "source": [
        "# As you can see, our simple model only has a single variable collection called\n",
        "# \"params\". So let's make this explicit:\n",
        "params = variables['params']\n",
        "\n",
        "# In an advanced usecase we might have different variable collections (e.g.\n",
        "# batch norm), and then we would need to treat them differently. In particular,\n",
        "# we would not want our optimizer to change anything other than \"params\". So\n",
        "# it's better for readability to split out \"params\" even in this simple case\n",
        "# where we could have treated all `variables` as params."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF04ELf5z_7j"
      },
      "source": [
        "# 3. Forward pass.\n",
        "log_probs, = model.apply({'params': params}, train_images[:1])\n",
        "# Note that the model returns log probabilities.\n",
        "plt.bar(range(10), jnp.exp(log_probs))\n",
        "jnp.exp(log_probs).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WOzLH9lSbVo"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Rewrite the model to use `nn.compact`.\n",
        "# Make sure that the model output is identical.\n",
        "# Make sure that the model weights are identical."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHbJvmNVy3Jc",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "# Defining the same model, this time in \"compact\" style.\n",
        "\n",
        "class Model2(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    x = x.reshape([batch_size, -1])\n",
        "    # If we didn't specify `name` we would get \"Dense_0\" by default.\n",
        "    # But we want identical names to compare weights below.\n",
        "    x = nn.Dense(features=10, name='dense')(x)\n",
        "    # We return normalized logits (\"log_probs\").\n",
        "    return nn.log_softmax(x)\n",
        "\n",
        "# It's exactly the same, both variables & outputs:\n",
        "model2 = Model2()\n",
        "variables2 = model2.init(rng, train_images[:1])\n",
        "log_probs2 = model2.apply(variables2, train_images[:1])\n",
        "\n",
        "# Verify weights & output are identical.\n",
        "# See what happens when you change the `rng` variable (by calling\n",
        "# `jax.random.split()` to derive a new key and use that one).\n",
        "sum(jax.tree_flatten(\n",
        "    jax.tree_multimap(lambda x, x2: jnp.abs(x - x2).sum(), variables, variables2)\n",
        ")[0]), sum(jax.tree_flatten(\n",
        "    jax.tree_multimap(lambda x, x2: jnp.abs(x - x2).sum(), log_probs, log_probs2)\n",
        ")[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMTOHzPAxjNg"
      },
      "source": [
        "#### 1.2 - Train + evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3QHZnUhg6V8"
      },
      "source": [
        "def evaluate(params):\n",
        "  log_probs = model.apply({'params': params}, test_images)\n",
        "  # Computes accuracy over entire test set in a single go.\n",
        "  return (log_probs.argmax(axis=-1) == test_labels.argmax(axis=-1)).mean()\n",
        "\n",
        "# Not surprisingly we perform rather badly withtout training:\n",
        "evaluate(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nzjBSGr4L62"
      },
      "source": [
        "def loss(params, images, labels):\n",
        "  log_probs = model.apply({'params': params}, images)\n",
        "  return -jnp.mean(jnp.sum(labels * log_probs, axis=-1))\n",
        "\n",
        "# Loss is a single scalar value.\n",
        "loss(params, train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njKtW6Ko5Ykn"
      },
      "source": [
        "# Note how we compose the function transformations. `jax.jit()` doesn't change\n",
        "# the output, but it will compile function when it's used for the first time\n",
        "# (and every time it's re-used with different shapes) and make it a lot faster.\n",
        "%time grads_fn = jax.jit(jax.grad(loss))\n",
        "\n",
        "# Beware that we don't specify `model` as an input parameter, so if we later\n",
        "# change the `model` variable, we need to make sure that the function is\n",
        "# recompiled (e.g. by re-executing the cell above), otherwise the compiled\n",
        "# version will still be using the old definition of `model`.\n",
        "# We will be fixing this issue further down under \"2.3 - Training reloaded\"."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNHFQBZeX2nT"
      },
      "source": [
        "# Compute the gradients. The function will be compiled the first time this is\n",
        "# executed. You can try re-executing this and the last cell alone or in tandem.\n",
        "# Note how the initial compilation is orders of magnitude slower than simply\n",
        "# executing the compiled function.\n",
        "%time grads = grads_fn(params, train_images, train_labels)\n",
        "\n",
        "# Gradients have same shape as parameters.\n",
        "jax.tree_map(jnp.shape, grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxFu6XQ3liF"
      },
      "source": [
        "%%time\n",
        "\n",
        "def train(params, *, epochs):\n",
        "  accuracies = [evaluate(params)]\n",
        "  for epoch in tqdm.trange(epochs):\n",
        "    grads = grads_fn(params, train_images, train_labels)\n",
        "    # Manually implement gradient descent.\n",
        "    params = jax.tree_multimap(\n",
        "        lambda param, grad: param - 0.01 * grad,\n",
        "        params, grads\n",
        "    )\n",
        "    accuracies.append(evaluate(params))\n",
        "  plt.plot(accuracies)\n",
        "  return params\n",
        "\n",
        "# This is very fast (~2 secs). Note that the final accuracy is still pretty low\n",
        "# because we're using a very simple optimization and because we use the entire\n",
        "# training dataset in every update step (instead of batching).\n",
        "# We'll write a more verbose training loop further down.\n",
        "trained_params = train(params, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5FQIRAJxnAI"
      },
      "source": [
        "#### 1.3 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IBVQUn3Y9dU"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Use `trained_params` to infer labels from images and visualize them."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "sVqrsUriAqBt"
      },
      "source": [
        "#@markdown solution\n",
        "i0, n = 999, 7\n",
        "for image, logit, axs in zip(\n",
        "    test_images[i0: i0+n],\n",
        "    model.apply({'params': trained_params}, test_images[i0: i0+n]),\n",
        "    zip(*plt.subplots(2, n, figsize=(3*n, 5))[1]),\n",
        "):\n",
        "  axs[0].imshow(image[:, :, 0])\n",
        "  plt.xticks(rotation=90)\n",
        "  axs[1].bar(list(map(info.features['label'].int2str, range(num_classes))), jnp.exp(logit))\n",
        "  axs[1].tick_params(labelrotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV_qwH_Yg6nn"
      },
      "source": [
        "### 2 - Vision Transformer in Flax\n",
        "\n",
        "In this section we will construct our own Vision Transformer. Note that our\n",
        "version is a bit simplified for readability (e.g. not including dropout).\n",
        "\n",
        "And of course we know that the Vision Transformer will fail when trained from\n",
        "scratch on a tiny dataset because it is lacking inductive bias. So the results\n",
        "we will get by training on `fashion_mnist` will be pretty bad, but the code in\n",
        "this section still illustrates the basic functioning.\n",
        "\n",
        "For actually using a Vision Transformer in practice, we would in most cases\n",
        "start with a pre-trained checkpoint -- and that's exactly what we will be doing\n",
        "in section\n",
        "[3 - Exploring pre-trained VITs](#scrollTo=d5IqJjkjg87a)\n",
        "below..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4HpAhWb6Q-"
      },
      "source": [
        "![Vision Transformer](https://github.com/google-research/vision_transformer/raw/master/vit_figure.png)\n",
        "\n",
        "Figure 1 from paper\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwnww2Agx8FJ"
      },
      "source": [
        "#### 2.1 - Patch embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfZy8i9ckiad"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  # Note that the \"patch size\" is a model parameter. It determines the leading\n",
        "  # dimension of the linear projection of the patches.\n",
        "  patch_size: int\n",
        "  hidden_dim: int\n",
        "  fake_init: Optional[bool] = False  # Useful to visualize the reshaping.\n",
        "\n",
        "  def get_init_kw(self):\n",
        "    return dict(\n",
        "        kernel_init=nn.initializers.ones, bias_init=nn.initializers.zeros\n",
        "    ) if self.fake_init else {}\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    n, h, w, c = x.shape\n",
        "    assert w % self.patch_size == h % self.patch_size == 0\n",
        "    # 1. Compute the grid - that would be 3x3 in above \"Figure 1\".\n",
        "    gw, gh = w // self.patch_size, h // self.patch_size\n",
        "    # 2. Reshape width and height into grid x patch_size.\n",
        "    x = x.reshape([n, gh, self.patch_size, gw, self.patch_size, c])\n",
        "    # 3. Transpose so we have [batch, gh, gw, h // patch, w // patch, channels]\n",
        "    x = x.transpose([0, 1, 3, 2, 4, 5])\n",
        "    # 4. Reshape so we're left with a batch of patches.\n",
        "    x = x.reshape([n, gh * gw, self.patch_size * self.patch_size * c])\n",
        "    # 5. Project to `hidden_dim`.\n",
        "    x = nn.Dense(features=self.hidden_dim, **self.get_init_kw())(x)\n",
        "    # 6. Add learnable position embeddings.\n",
        "    posembed = self.param('posembed', nn.initializers.zeros, (1,) + x.shape[1:])\n",
        "    return x + posembed\n",
        "\n",
        "\n",
        "patch_embedding = PatchEmbedding(patch_size=4, hidden_dim=3, fake_init=True)\n",
        "params = patch_embedding.init(rng, jnp.ones([1, 8, 8, 1]))['params']\n",
        "\n",
        "# Note on weight matrix of linear projection: (16, 3) because we have a single\n",
        "# channel and patch_size=4, so a patch has 4*4*1 scalar values as input. Every\n",
        "# patch will be projected into a single vector of size 3.\n",
        "# Note on the position embeddings: (1, 4, 3) because we have a grid with two\n",
        "# rows and two columns. A single position embedding is a vector of `hidden_dim`.\n",
        "jax.tree_map(jnp.shape, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESM5U45ovTWa"
      },
      "source": [
        "# Create a fake image with 4 patches of constant value.\n",
        "checkered = jnp.repeat(jnp.repeat(jnp.array([[1., 2], [3, 4]]), 4, axis=0), 4, axis=1)\n",
        "checkered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ceblKPwGKD"
      },
      "source": [
        "# Thanks to `fake_init=True` every value in hidden_dim will be the summation\n",
        "# of the individual 16 pixel values above.\n",
        "# Note that embedded is now a sequence (of \"tokens\") with length grid width *\n",
        "# grid height.\n",
        "embedded = patch_embedding.apply({'params': params}, checkered[None, ..., None])\n",
        "embedded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW47scupfxV2"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Rewrite `PatchEmbedding` to use a single convolution instead of the reshape +\n",
        "# linear projection. Inspect the weights and make sure you get the same values\n",
        "# when applying the module.\n",
        "# You'll want to use the `nn.Conv()` module:\n",
        "# https://flax.readthedocs.io/en/latest/_modules/flax/linen/linear.html#Conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugx_fcFBLMjg",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  patch_size: int\n",
        "  hidden_dim: int\n",
        "  fake_init: Optional[bool] = False  # Useful to visualize the reshaping.\n",
        "\n",
        "  def get_init_kw(self):\n",
        "    return dict(\n",
        "        kernel_init=nn.initializers.ones, bias_init=nn.initializers.zeros\n",
        "    ) if self.fake_init else {}\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    n, h, w, c = x.shape\n",
        "    x = nn.Conv(\n",
        "        features=self.hidden_dim,\n",
        "        # Using kernel_size=strides=patch_size will apply the same linear\n",
        "        # projection for every patch.\n",
        "        kernel_size=(self.patch_size, self.patch_size),\n",
        "        strides=(self.patch_size, self.patch_size),\n",
        "        **self.get_init_kw(),\n",
        "    )(x)\n",
        "    x = x.reshape([n, -1, self.hidden_dim])\n",
        "    posembed = self.param('posembed', nn.initializers.zeros, (1,) + x.shape[1:])\n",
        "    return x + posembed\n",
        "\n",
        "\n",
        "patch_embedding = PatchEmbedding(patch_size=4, hidden_dim=3, fake_init=True)\n",
        "params = patch_embedding.init(rng, jnp.ones([1, 8, 8, 1]))['params']\n",
        "print(jax.tree_map(jnp.shape, params))\n",
        "patch_embedding.apply({'params': params}, checkered[None, ..., None])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtTYx_9-Bp1V"
      },
      "source": [
        "#### 2.2 - Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH2spSjfJYzX"
      },
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "  mlp_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    x = nn.LayerNorm()(inputs)\n",
        "    # This is the crucial operation. It allows every token to attend to all\n",
        "    # other tokens (in the same example).\n",
        "    # It's a very powerful and generic operation, unfortunately its runtime\n",
        "    # complexity is also qudratic wrt sequence length (which in turn is\n",
        "    # quadratic wrt both image_size and 1/patch_size).\n",
        "    x = nn.MultiHeadDotProductAttention(self.num_heads, deterministic=True)(x, x)\n",
        "\n",
        "    x = x + inputs  # Residual.\n",
        "\n",
        "    # MLP with single hidden layer:\n",
        "    x = nn.LayerNorm()(x)\n",
        "    hidden_dim = x.shape[-1]\n",
        "    y = nn.Dense(self.mlp_dim)(x)\n",
        "    y = nn.gelu(x)\n",
        "    y = nn.Dense(hidden_dim)(x)\n",
        "\n",
        "    return x + y  # Residual.\n",
        "\n",
        "\n",
        "transformer_layer = TransformerLayer(mlp_dim=128, num_heads=4)\n",
        "\n",
        "# One sequence, of length 16, 64 hidden dimensions.\n",
        "seqs = jnp.ones([1, (28 // 7) ** 2, 64])\n",
        "params = transformer_layer.init(rng, seqs)['params']\n",
        "transformed_seqs = transformer_layer.apply({'params': params}, seqs)\n",
        "\n",
        "jax.tree_map(jnp.shape, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfiWqNH4whhG"
      },
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "\n",
        "  patch_size: int\n",
        "  hidden_dim: int\n",
        "\n",
        "  mlp_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  layers: int\n",
        "  num_classes: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "\n",
        "    # Embeds patches & adds position embedding.\n",
        "    x = PatchEmbedding(\n",
        "        patch_size=self.patch_size,\n",
        "        hidden_dim=self.hidden_dim\n",
        "    )(x)\n",
        "\n",
        "    # Transformer encoder.\n",
        "    for layer in range(self.layers):\n",
        "      x = TransformerLayer(\n",
        "          mlp_dim=self.mlp_dim,\n",
        "          num_heads=self.num_heads,\n",
        "      )(x)\n",
        "\n",
        "    # Different classification heads are possible. Here we use the simplest\n",
        "    # possible, by first taking the average across the sequence dimension, and\n",
        "    # then adding a single linear projection on top of it.\n",
        "    # This is different from the classification approach used in the original\n",
        "    # ViT paper, where a special \"cls\" token is prepended to the sequence and a\n",
        "    # classification MLP is applied on top of that special token at the end of\n",
        "    # the model. It has been shown (see e.g. Figure 9 in the appendix of\n",
        "    # https://arxiv.org/abs/2010.11929v2) that simple average pooling can\n",
        "    # perform as well, if the learning rate is tuned appropriately.\n",
        "    x = x.mean(axis=-2)\n",
        "    x = nn.Dense(num_classes)(x)\n",
        "    return nn.log_softmax(x)\n",
        "\n",
        "\n",
        "# Given that we have image_size=28, a patch_size=7 gives us a 4x4 grid.\n",
        "model = VisionTransformer(patch_size=7, hidden_dim=64, mlp_dim=128, num_heads=4, layers=4, num_classes=10)\n",
        "params = model.init(rng, train_images[:1])['params']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H5HZFKbpeom"
      },
      "source": [
        "# Let's examine the parameters in some more details:\n",
        "df = pd.DataFrame([\n",
        "    dict(path1=k[0], path2=k[1], path3='/'.join(k[2:]), shape=v.shape, params=np.prod(v.shape))\n",
        "    for k, v in flax.traverse_util.flatten_dict(params.unfreeze()).items()\n",
        "])\n",
        "print(f'total {df.params.sum():,}')\n",
        "df.set_index(['path1', 'path2', 'path3']).sort_index().T.style.set_table_styles([\n",
        "    # Improve display for multi-level column headers:\n",
        "    {'selector': 'th', 'props': [('background', '#eee')]}\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "licMjF_Wnvdx"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# What happens to the parameters when you change to patch_size=4?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLLTYAqAnOu_",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "model2 = VisionTransformer(patch_size=4, hidden_dim=64, mlp_dim=128, num_heads=4, layers=4, num_classes=10)\n",
        "params2 = model2.init(rng, train_images[:1])['params']\n",
        "\n",
        "df2 = pd.DataFrame([\n",
        "    dict(path1=k[0], path2=k[1], path3='/'.join(k[2:]), shape=v.shape, params=np.prod(v.shape))\n",
        "    for k, v in flax.traverse_util.flatten_dict(params2.unfreeze()).items()\n",
        "])\n",
        "# Same overall number of parameters...\n",
        "print(f'total {df2.params.sum():,}')\n",
        "# ...because the change in parameters of the position embedding and the\n",
        "# embedding projection matrix exactly cancel out. That's because 28==7*4. Note\n",
        "# that changing to e.g. kernel_size=14 does indeed show a (small) change in\n",
        "# number of parameters.\n",
        "df.merge(df2, on=['path1', 'path2', 'path3']).query('params_x!=params_y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tPheHmRszQQ"
      },
      "source": [
        "Let me reiterate this: Number of parameters does not actually matter that much.\n",
        "It's the total compute that matters for Vision Transformers.\n",
        "\n",
        "You can see this for example in Figure 8 from paper\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929):\n",
        "\n",
        "![Figure 8](https://i.imgur.com/NZIkCQk.png)\n",
        "\n",
        "We get almost the same increase in performance when changing `patch_size` as we\n",
        "get by scaling other parameters. But as we show above the number of parameters hardly\n",
        "changes!\n",
        "\n",
        "Another paper to bookmark to read after this tutorial:\n",
        "[Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited](https://arxiv.org/abs/2003.02139)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbgZKcIax0cI"
      },
      "source": [
        "#### 2.3 - Training, reloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0KrkJh4IfOH"
      },
      "source": [
        "# Our simple training loop from above \"1.2 - Train + evaluate\" won't actually\n",
        "# work for training a Vision Transformer. Check out the new improved training\n",
        "# loop:\n",
        "\n",
        "def train(model, params, *, ds, epochs, batch_size=128, lr=0.01):\n",
        "\n",
        "  # Using a learning rate schedule with a linear warmup stabilizes training\n",
        "  # a lot.\n",
        "  steps = int(epochs * num_train / batch_size)\n",
        "  warmup_steps = int(.1 * steps)\n",
        "  lr_schedule = optax.join_schedules([\n",
        "      optax.linear_schedule(0, lr, warmup_steps),\n",
        "      optax.cosine_decay_schedule(lr, steps - warmup_steps),\n",
        "  ], boundaries=[warmup_steps])\n",
        "\n",
        "  tx = optax.chain(\n",
        "      # Gradient clipping stabilizes training. Not really needed for these small\n",
        "      # toy models, but useful trick for training larger Vision Transformers.\n",
        "      optax.clip_by_global_norm(1.0),\n",
        "      optax.adam(lr_schedule),\n",
        "  )\n",
        "  opt_state = tx.init(params)\n",
        "\n",
        "  # We redefine `loss_fn()` to make sure that it always uses the correct model\n",
        "  # (which is now explicitely provided as a function argument instead of using\n",
        "  # a notebook global as before).\n",
        "  def loss_fn(params, images, labels):\n",
        "    logits = model.apply({'params': params}, images)\n",
        "    return -jnp.mean(jnp.sum(labels * logits, axis=-1))\n",
        "\n",
        "  # We want to JIT compile as large a part of our program as possible. Usually\n",
        "  # the `train_step()` is perfectly suited for this, as it only involves pure\n",
        "  # computation, and all data loading/logging is done outside of this function.\n",
        "  @jax.jit\n",
        "  def train_step(params, opt_state, images, labels):\n",
        "    grads = jax.grad(loss_fn)(params, images, labels)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "  evaluate_ = jax.jit(evaluate)\n",
        "  accuracies = [evaluate_(params)]\n",
        "  # Most important change: Train on minibatches instead of using the entire\n",
        "  # training dataset. This improves the performance a lot, while also allowing\n",
        "  # for larger models.\n",
        "  for step, (images, labels) in zip(\n",
        "      tqdm.trange(steps),\n",
        "      pp(ds.repeat().batch(batch_size).shuffle(1000)),\n",
        "  ):\n",
        "    params, opt_state = train_step(params, opt_state, images, labels)\n",
        "    if (step + 1) % (steps // 10) == 0 or step + 1 == steps:\n",
        "      accuracies.append(evaluate_(params))\n",
        "\n",
        "  plt.plot(accuracies)\n",
        "  print('final accuracy', accuracies[-1])\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcO3T4u7mTmH"
      },
      "source": [
        "# So let's try a Vision Transformer on this toy dataset.\n",
        "# As said in the introduction of this section, this is far too little data to\n",
        "# get to any useful performance! Please don't train Vision Transformers from\n",
        "# scratch with MNIST-scaled problems...\n",
        "%%time\n",
        "model = VisionTransformer(patch_size=7, hidden_dim=64, mlp_dim=128, num_heads=4, layers=4, num_classes=10)\n",
        "params = model.init(rng, train_images[:1])['params']\n",
        "trained_params = train(model, params, ds=ds['train'], epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2lPl-NmuV9C"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Modify hyper parameters and the training loop to get a feel of how the\n",
        "# result changes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCmFfUn4wgsL",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "\n",
        "# There's no solution for this exercise."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5IqJjkjg87a"
      },
      "source": [
        "### 3 - Exploring pre-trained ViTs\n",
        "\n",
        "In this section we will explore pre-trained checkpoints from the official ViT\n",
        "repository\n",
        "\n",
        "https://github.com/google-research/vision_transformer/\n",
        "\n",
        "Note that the repository has two Colabs for further study of Vision Transformers\n",
        "(maybe after this tutorial?)\n",
        "\n",
        "- [vit_jax.ipynb](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb) -\n",
        "  Mimicks the main training loop in Colab, with lots of annotations. Useful to\n",
        "  understand single host data-parallelism using JAX.\n",
        "- [vit_jax_augreg.ipynb](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb) -\n",
        "  Allows you to interactively explore the 50k ViT checkpoints and has code to\n",
        "  fine-tune them on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLrOh5U_QMho"
      },
      "source": [
        "#### 3.0 - Download repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx3xr2X9QOSI"
      },
      "source": [
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n",
        "!pip install -qr vision_transformer/vit_jax/requirements.txt\n",
        "import sys\n",
        "if './vision_transformer' not in sys.path:\n",
        "  sys.path.append('./vision_transformer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOH-_cDwNgWu"
      },
      "source": [
        "#### 3.1 - Exploring checkpoints\n",
        "\n",
        "For a detailed description of the checkpoints, refer to the paper\n",
        "\n",
        "[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)\n",
        "\n",
        "We'll use Pandas to explore the checkpoints. You might want to have their\n",
        "[Cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
        "ready..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3MKXlMbNqE4"
      },
      "source": [
        "from vit_jax import checkpoint\n",
        "df = checkpoint.get_augreg_df()\n",
        "print(checkpoint.get_augreg_df.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcL_7waXxX39"
      },
      "source": [
        "# Note that columns NOT starting with \"adapt_\" are about the pre-training.\n",
        "# For example, we have 756 pretrained checkpoints.\n",
        "len(df.filename.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtLBXV58xmwO"
      },
      "source": [
        "# We can select a single pre-trained checkpoint by its AugReg settings:\n",
        "filename = df.query(\n",
        "    'name==\"B/32\"'  # Choose a single model.\n",
        "    'and ds==\"i21k\"'  # Upstream dataset -- i21k is best.\n",
        "    'and aug==\"light1\"'  # Data augmentation.\n",
        "    'and wd==0.1 and sd==0.0'  # Model regularization.\n",
        ").filename.unique()[0]\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhkBNVp_EYJH"
      },
      "source": [
        "# That's exactly the basename of the checkpointinside the storage bucket\n",
        "# gs://vit_models/augreg\n",
        "!gsutil ls -lh gs://vit_models/augreg/{filename}.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-BYRi15HqW0"
      },
      "source": [
        "Wondering how to select the \"best\" model? That really depends on what you want\n",
        "to achieve. In general larger (=slower) models lead to better results:\n",
        "\n",
        "![VTAB results different models](https://i.imgur.com/TlM0u9F.png)\n",
        "\n",
        "Figure 3 from paper\n",
        "[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnAVnAAYG68c"
      },
      "source": [
        "# Note by the way that above checkpoint is the setting yielding the best\n",
        "# upstream validation accuracy:\n",
        "df.query(\n",
        "    # Only focus on a single model\n",
        "    'name==\"B/32\"'\n",
        "    'and ds==\"i21k\"'\n",
        ").sort_values('final_val').iloc[-1].filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A05NWs2zJHm6"
      },
      "source": [
        "# Note that we still have lots of rows in the dataframe with this one upstream\n",
        "# checkpoint because the dataframe also includes data for the fine-tunings:\n",
        "len(df.query(f'filename==\"{filename}\"'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naSrY5GqIfrX"
      },
      "source": [
        "# Now let's find the best downstream adaptation of this checkpoint on cifar100:\n",
        "adapt_filename = df.query(\n",
        "    f'filename==\"{filename}\" and adapt_ds==\"cifar100\"'\n",
        ").sort_values('adapt_final_val').iloc[-1].adapt_filename\n",
        "# Note that adaptation parameters are encoded in the filename after the \"--\"\n",
        "adapt_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnOBYrvKNh05"
      },
      "source": [
        "#### 3.2 - Loading checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY6gHtlVKF4W"
      },
      "source": [
        "# From now on we're going to use code from the repository that we downloaded\n",
        "# above.\n",
        "from vit_jax import checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6olN0yynNqq2"
      },
      "source": [
        "# Let's open that file in an editor\n",
        "from google.colab import files\n",
        "files.view('vision_transformer/vit_jax/checkpoint.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1LXWiJhKh4e"
      },
      "source": [
        "# And let's auto-reload any changes you make in that editor.\n",
        "# Note though that the changes WILL NOT be persisted because the local hard\n",
        "# drive will be reset when the VM is shut down (after some idle time).\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd7ycmx-KihY"
      },
      "source": [
        "params = checkpoint.load(f'gs://vit_models/augreg/{adapt_filename}.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_r3Yeo8K8RZ"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Check out the library code in the right hand editor while the checkpoint is\n",
        "# being loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4TYjXiLEOC"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# How can you deduce what was the resolution used for fine-tuning this model by\n",
        "# looking only at the parameters?\n",
        "# Note that the parameter structure is slightly different. You can read the\n",
        "# models.py file, or simply inspect `params[...].keys()` to find the telling\n",
        "# shapes...\n",
        "\n",
        "# files.view('vision_transformer/vit_jax/models.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-IHk4rgLe-j",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "\n",
        "patch_size, patch_size, channels, hidden_dims = (\n",
        "    params['embedding']['kernel'].shape)\n",
        "_, sequence_length, hidden_dims = (\n",
        "    params['Transformer']['posembed_input']['pos_embedding'].shape)\n",
        "grid_size = (sequence_length - 1) ** .5  # Subtract the \"cls\" token.\n",
        "grid_size * patch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OVtorG0OHtB"
      },
      "source": [
        "# Visualization of the similarity between the learned patch embeddings.\n",
        "\n",
        "def figure9(params):\n",
        "  \"\"\"Plots embedding pairwise cosine similarity, see Figure 9 in ViT paper.\"\"\"\n",
        "  patches = params['Transformer']['posembed_input']['pos_embedding'][0]\n",
        "  patches = patches[1:]  # Remove cls token.\n",
        "  width = height = int(len(patches) ** .5)\n",
        "  _, axs = plt.subplots(height, width, figsize=(width, height))\n",
        "  for patch1, ax in zip(patches, np.array(axs).flatten()):\n",
        "    ax.matshow(np.array([\n",
        "        patch1.dot(patch2) / np.linalg.norm(patch1) / np.linalg.norm(patch2)\n",
        "        for patch2 in patches\n",
        "    ]).reshape([height, width]))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "\n",
        "figure9(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63mOQrb1TJ6F"
      },
      "source": [
        "ds, info = tfds.load('cifar100', with_info=True)\n",
        "num_classes = info.features['label'].num_classes\n",
        "int2str = info.features['label'].int2str\n",
        "d = next(iter(ds['train']))\n",
        "image = d['image']\n",
        "plt.matshow(image)\n",
        "image.shape, int2str(d['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT-esI2fTTk5"
      },
      "source": [
        "from vit_jax.configs import models as models_config  # Model configurations.\n",
        "from vit_jax import models  # Actual model code.\n",
        "files.view('vision_transformer/vit_jax/configs/models.py')\n",
        "files.view('vision_transformer/vit_jax/models.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08TJwalUTVG2"
      },
      "source": [
        "# Instantiate the matching model from the repo.\n",
        "config = models_config.AUGREG_CONFIGS[adapt_filename.split('-')[0]]\n",
        "vit = models.VisionTransformer(**config, num_classes=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohq-8VS2qfOW"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# Modify `image` according to input_pieline.py and predict logits using `model`.\n",
        "files.view('vision_transformer/vit_jax/input_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKw8gX7rWL7D",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "# Expected input resolution: 224 pixels.\n",
        "# Input is expected to be normalized -1..1 range.\n",
        "image_ = jnp.array(tf.image.resize(image, [224, 224]) / 127.5 - 1)\n",
        "logits = vit.apply({'params': params}, image_[None], train=False)\n",
        "\n",
        "plt.figure(figsize=(17, 3))\n",
        "plt.bar(list(map(int2str, range(num_classes))), logits[0])\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N_dUjGwNmn_"
      },
      "source": [
        "#### 3.3 - Fine-tuning checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyispr7JsZS8"
      },
      "source": [
        "# Instead of scaling the image as we did in \"3.2 - Loading checkpoints\", we can\n",
        "# instead re-generate the position embeddings to match the new grid size. We\n",
        "# provide the function `checkpoint.load_pretrained()` for doing this:\n",
        "config = models_config.AUGREG_CONFIGS[filename.split('-')[0]]\n",
        "vit = models.VisionTransformer(**config, num_classes=num_classes)\n",
        "\n",
        "# We need \"template\" parameters that depend on model & image size.\n",
        "init_params = vit.init(rng, np.ones([1, 5*32, 5*32, 3]), train=False)['params']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N4zq5J3tSSS"
      },
      "source": [
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Show logging messages for rescaling etc.\n",
        "\n",
        "params = checkpoint.load_pretrained(\n",
        "    pretrained_path=f'gs://vit_models/augreg/{filename}.npz',\n",
        "    init_params=init_params,\n",
        "    model_config=config,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAMpUcdOt-y3"
      },
      "source": [
        "# The rescaled position embeddings still have the expected similarity structure.\n",
        "figure9(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGtCWoe31dNv"
      },
      "source": [
        "del params  # Only used for demonstrating scaling."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbkAuzMM02UE"
      },
      "source": [
        "# For the fine-tuning let's do it a bit differently from above: Instead of using\n",
        "# our hand-rolled `train()` we'll instead be using the function\n",
        "# `train_and_evaluate()` provided in the repo:\n",
        "files.view('vision_transformer/vit_jax/train.py')\n",
        "from vit_jax import train as train_lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "horoM7Ot1rrX"
      },
      "source": [
        "# Let's load a config that also contains default parameters for training...\n",
        "files.view('vision_transformer/vit_jax/configs/augreg.py')\n",
        "from vit_jax.configs import augreg as augreg_config\n",
        "config = augreg_config.get_config(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j2oq6gQ05ss"
      },
      "source": [
        "# ...and then adapt some of those config parameters:\n",
        "\n",
        "# Note: We're not using fashion_mnist here because it's really a toy dataset\n",
        "# and its \"images\" are not quite representative.\n",
        "# Check out the TFDS catalog for more image classification datasets:\n",
        "# https://www.tensorflow.org/datasets/catalog/overview\n",
        "config.dataset = 'oxford_flowers102'\n",
        "config.pp.train = 'train'\n",
        "config.pp.test = 'test'\n",
        "\n",
        "# Some more parameters that you will often want to set manually.\n",
        "# For example for VTAB we used steps={500, 2500} and lr={.001, .003, .01, .03}\n",
        "config.base_lr = 0.01\n",
        "config.shuffle_buffer = 1000\n",
        "config.total_steps = 100\n",
        "config.warmup_steps = 10\n",
        "config.accum_steps = 4  # Might need to be adjusted depending on model.\n",
        "config.pp['crop'] = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6uaCGDXLmr"
      },
      "source": [
        "# Launch tensorboard before training - maybe click \"reload\" during training.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./workdirs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Ee2-GzZ9sl"
      },
      "source": [
        "# Create a new temporary workdir.\n",
        "import time\n",
        "workdir = f'./workdirs/{int(time.time())}'\n",
        "workdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cxCM3LcwwX4"
      },
      "source": [
        "# This cell takes ~10 min to finish (using B/32 & flowers102).\n",
        "opt = train_lib.train_and_evaluate(config, workdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPDlo0ZTD_-q"
      },
      "source": [
        "# Note that function returns a flax.optim Optimizer storing params as \"target\".\n",
        "trained_params = opt.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIQEozJ1-PEI"
      },
      "source": [
        "# Create model with same settings used for training above.\n",
        "vit = models.VisionTransformer(\n",
        "    **config.model, num_classes=len(trained_params['head']['bias']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NawT_RCjFQWM"
      },
      "source": [
        "# Create input pipeline with same settings as used for training above.\n",
        "from vit_jax import input_pipeline\n",
        "\n",
        "ds_test = input_pipeline.get_data_from_tfds(config=config, mode='test')\n",
        "int2str = tfds.builder(config.dataset).info.features['label'].int2str\n",
        "\n",
        "batch = next(iter(ds_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0cS9l-GFOzg"
      },
      "source": [
        "# Plot top predictions from learned parameters.\n",
        "i0, n = 12, 7\n",
        "images = batch['image'][0, i0: i0+n]\n",
        "for image, logit, axs in zip(\n",
        "    images,\n",
        "    vit.apply({'params': trained_params}, images, train=False),\n",
        "    zip(*plt.subplots(2, n, figsize=(3*n, 5))[1]),\n",
        "):\n",
        "  axs[0].imshow(image / 2 + 0.5)\n",
        "  idx = logit.argsort()[::-1][:10]\n",
        "  plt.xticks(rotation=90)\n",
        "  axs[1].bar(list(map(int2str, idx)), logit[idx])\n",
        "  axs[1].tick_params(labelrotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0EyF3u72RLy"
      },
      "source": [
        "# YOUR ACTION REQUIRED:\n",
        "# While the model is training, you can check out the training code in the\n",
        "# repository (in file \"train.py\" on the right side).\n",
        "# You could try to adapt our `train()` function from above to make it work with\n",
        "# the pre-trained checkpoints.\n",
        "# 1. In particular, you'll need to handle `train` correctly and provide PRNG\n",
        "#    keys for dropout.\n",
        "# 2. Note that the returned values are simply logits, not log_probs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdaVam52olt",
        "cellView": "form"
      },
      "source": [
        "#@markdown solution\n",
        "\n",
        "# There's no solution for this exercise."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaV2MrQD3Ihi"
      },
      "source": [
        "If you're interested in fine-tuning for real, you probably want to have a look\n",
        "at the\n",
        "[vit_jax_augreg.ipynb](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb)\n",
        "Colab that also contains code to fine-tune on data that is not in tfds format.\n",
        "\n",
        "Do you prefer PyTorch even after finishing this tutorial? The good news is that\n",
        "you can directly import any of these checkpoints using the `timm` package. Check\n",
        "out the Colab above for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biUGL9bRBLDa"
      },
      "source": [
        "### 4 - Wrapup\n",
        "\n",
        "It would be great if you could tell me what you liked (or not) about this\n",
        "tutorial, so I can make it better for the next iteration!\n",
        "\n",
        "[**Feedback form** (1 minute version)](https://forms.gle/aritdqKJVaMPDYxD6)"
      ]
    }
  ]
}